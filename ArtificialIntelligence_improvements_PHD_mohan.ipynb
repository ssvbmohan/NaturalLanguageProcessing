{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial intelligence models built are:\n",
    " ### 1. cnn( with various layers)\n",
    " ### 2. lstm(with various layers and optimizers)\n",
    " ### 3. embedding values of cnn taken into svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Jhansi Rani.SSV_BALARAM_MOH\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Dr. Jhansi Rani.SSV_BALARAM_MOH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Dr. Jhansi Rani.SSV_BALARAM_MOH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Dr. Jhansi Rani.SSV_BALARAM_MOH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Dr. Jhansi Rani.SSV_BALARAM_MOH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Dr. Jhansi Rani.SSV_BALARAM_MOH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Dr. Jhansi Rani.SSV_BALARAM_MOH\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import pandas as pd     # for reading data operations\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer          # for tokenizing text\n",
    "from keras.preprocessing.sequence import pad_sequences  # for padding sentences with zeros. To make the sentence length same\n",
    "from keras.utils import to_categorical                  # for one-hot encoding of the labels\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Input, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Embedding, GlobalAvgPool1D\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = '4000reviews_preprocessed.csv'\n",
    "infile = open(filename,'rb')\n",
    "review3000_dt = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final_preprocessed.csv'\n",
    "infile = open(filename,'rb')\n",
    "final = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_processed_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>I wish they kept the same cast...probably woul...</td>\n",
       "      <td>0</td>\n",
       "      <td>wish kept cast probably would cost would worth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment  \\\n",
       "234  I wish they kept the same cast...probably woul...          0   \n",
       "\n",
       "                                 review_processed_1  \n",
       "234  wish kept cast probably would cost would worth  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review3000_dt.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review3000_dt['sentiment'] = review3000_dt['sentiment'].replace({0:1, 1:0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review3000_r_dt = review3000_dt.drop(labels=['review_processed_1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review3000_rp1_dt = review3000_dt.drop(labels=['review'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_rp1, X_test_rp1, y_train_rp1, y_test_rp1 = train_test_split(review3000_dt['review_processed_1'],review3000_dt['sentiment'],test_size=0.20,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(review3000_dt['review'],review3000_dt['sentiment'],test_size=0.20,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original much better music performance seemed lacking heart seemed like flat also felt like humor lightheartedness original missing opinion'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review3000_dt.review_processed_1[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 5000\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the sequence lengths, max number of words and embedding dimensions\n",
    "MAX_SEQUENCE_LENGTH = 5000   # Sequence length of each sentence. If more, crop. If less, pad with zeros\n",
    "MAX_NB_WORDS = 10000       # Top 20000 frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3578 unique tokens.\n",
      "(2400, 5000)\n",
      "(600, 5000)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)   # get the frequently occuring words\n",
    "tokenizer.fit_on_texts(X_train_rp1)           \n",
    "train_sequences = tokenizer.texts_to_sequences(X_train_rp1)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test_rp1)\n",
    "\n",
    "word_index = tokenizer.word_index               # dictionary containing words and their index\n",
    "# print(tokenizer.word_index)                   # print to check\n",
    "print('Found %s unique tokens.' % len(word_index)) # total words in the corpus\n",
    "X_train_dt = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) # get only the top frequent words on train\n",
    "X_test_dt = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)   # get only the top frequent words on test\n",
    "\n",
    "print(X_train_dt.shape)\n",
    "print(X_test_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4587 unique tokens.\n",
      "(2400, 5000)\n",
      "(600, 5000)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)   # get the frequently occuring words\n",
    "tokenizer.fit_on_texts(X_train_r)           \n",
    "train_sequences = tokenizer.texts_to_sequences(X_train_r)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test_r)\n",
    "final_seq = tokenizer.texts_to_sequences(final.Review)\n",
    "word_index = tokenizer.word_index               # dictionary containing words and their index\n",
    "# print(tokenizer.word_index)                   # print to check\n",
    "print('Found %s unique tokens.' % len(word_index)) # total words in the corpus\n",
    "X_train_r_dt = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) # get only the top frequent words on train\n",
    "X_test_r_dt = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)   # get only the top frequent words on test\n",
    "final_dt = pad_sequences(final_seq,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(X_train_r_dt.shape)\n",
    "print(X_test_r_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/4\n",
      "2400/2400 [==============================] - 425s 177ms/step - loss: 0.5993 - acc: 0.6800 - val_loss: 0.5001 - val_acc: 0.7433\n",
      "Epoch 2/4\n",
      "2400/2400 [==============================] - 376s 157ms/step - loss: 0.3619 - acc: 0.8471 - val_loss: 0.4282 - val_acc: 0.8183\n",
      "Epoch 3/4\n",
      "2400/2400 [==============================] - 361s 150ms/step - loss: 0.1995 - acc: 0.9308 - val_loss: 0.4854 - val_acc: 0.8183\n",
      "Epoch 4/4\n",
      "2400/2400 [==============================] - 375s 156ms/step - loss: 0.1236 - acc: 0.9596 - val_loss: 0.5445 - val_acc: 0.8067\n",
      "600/600 [==============================] - 21s 35ms/step\n",
      "Test score: 0.544458407908678\n",
      "Test accuracy: 0.8066666632890701\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_r_dt, y_train_r,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test_r_dt, y_test_r))\n",
    "score, acc = model.evaluate(X_test_r_dt, y_test_r, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9707943925233644\n",
      "0.7251184834123222\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict_classes(X_train_r_dt)\n",
    "pred_test = model.predict_classes(X_test_r_dt)\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_train=f1_score( y_train_r, pred_train, labels=None, pos_label=1, average='binary')\n",
    "f1_test=f1_score( y_test_r, pred_test, labels=None, pos_label=1, average='binary')\n",
    "print(f1_train)\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['sentiment']=final_predict_m5\n",
    "\n",
    "f11=final.drop(labels=['Review','review_processed_1'], axis=1)\n",
    "\n",
    "f11.sample(20)\n",
    "\n",
    "f11.to_csv(\"model32.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    871\n",
       "1    329\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f11.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
